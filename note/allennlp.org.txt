
* token_indexers and its keys
  token_indexers is a dictionary of pairs whose key is a name and value is an indexer
** with Embeddeer
   token_indexers' key is used by Embedder such as BasicTextFieldEmbedder
** with Field.get_padding_lengths
   token_indexers' key is used by Field.get_padding_lengths such as TextField.get_padding_lengths
   - TextField.get_padding_lengths
     The key of padding_lengths is combination of keys of token_indexers and TextField._indexed_tokens,
     where the latter is always "tokens" when Field._indexed_tokens made by SingleIdTokenIndexer.tokens_to_indices in Field.index ("tokens" is hard-coded in SingleIdTokenIndexer.tokens_to_indices).

* Field.as_tensor
  it returns a nested dictionary that has the same keys of padding_lengths returned by Field.get_padding_lengths

* TextField vs. LabelField
  TextField and LabelField use different implementations of 'batch_tensors'
  LabelField inherits 'batch_tensors' from Field, so it directly returns 'torch.tensor'

* FromParam
  https://guide.allennlp.org/using-config-files#4
  https://docs.allennlp.org/main/api/models/model/#model-objects
  documentaion of some class, such as Model or Trainer, describes which argument should not get an "entry" in configuration file
  e.g. Model.vocab should not get an entry of a configuration file

* Registrable.register and Lazy
  when a constructor with Lazy type annotation is registered by "Registrable.register", the object can be created with some complex process

* Early importing Registerable classes
  https://guide.allennlp.org/using-config-files#7
  - argument "--include-package"
  - file ".allennlp_plugins"

* Random seed example
  https://guide.allennlp.org/hyperparameter-optimization#3
  {numpy_seed: seed,
   pytorch_seed: seed,
   random_seed: seed, ...}

* processing text to tensors
  - an Instance consists of Fields
  - the length information of a Field can be obtained by Field.get_padding_lengths()
    e.g.
    >>> padding_lengths = field.get_padding_lengths()
    - however, LabelField.get_padding_lengths returns an empty dictionary, so it doesn't affect Instance.get_padding_lengths
  - for batching, multiple padding_lengths values are merged into an unified padding_lengths
    which has the maximum value for each position
    - the number of padding_lengths values before merging = batch size
  - a Field can be converted into a dictionary of tensors by Field.as_tensor when a padding_lengths is given
    e.g.
    >>> tensor_dict = field.as_tensor(padding_lengths)
  - multiple tensor_dicts can be merged into a batched tensor_dict
    e.g.
    >>> batched_tensor_dict = field.batch_tensors([tensor_dict1, tensor_dict2, ...])
  - Instance also has methods Instance.get_padding_lengths and Instance.as_tensor_dict,
    which are used for converting an Instance into tensor_dict
    - Instance.get_padding_lengths uses Field.get_padding_lengths
      Instance.as_tensor_dict uses Field.as_tensor
    - however, Instance.get_padding_lengths doesn't include the lengths of LabelField
  - TextFieldEmbedder, such as BasicTextFieldEmbedder, converts tensor_dict into torch.Tensor
    - BasicTextFieldEmbedder consists of several TokenEmbedders
      where each TokenEmbedder is specialized for embedding each Field
      - examples of TokenEmbedders include
	- Embedding
	- TokenCharactersEncoder
	- PretrainedTransformerEmbedder
	- and so on

