
* Usage
** sinfo
   Check recource

   [Command]
   $ sinfo

   [Output]
   PARTITION  CPUS(A/I/O/T)  STATE  NODELIST  NODES GRES             MEMORY  TIMELIMIT   AVAIL_FEATURES
   2080ti*    4/76/0/80      mix    n[1-4]    4     gpu:GTX2080Ti:8  120000  3-00:00:00  (null)        
   titanrtx   2/38/0/40      mix    n[5,7]    2     gpu:TITANRTX:4   250000  3-00:00:00  (null)        
   titanrtx   0/88/0/88      idle   n[6,8-9]  3     gpu:TITANRTX:4   250000  3-00:00:00  (null)        
   titanxp    16/24/0/40     mix    n[10-11]  2     gpu:TITANXP:8    120000  3-00:00:00  (null)        
   titanxp    0/40/0/40      idle   n[12-13]  2     gpu:TITANXP:8    120000  3-00:00:00  (null)        

   [Details]
   - There exist 5 partitions, where each partition has several nodes
   - Information of CPUS indicates Allocated/Idle/Other/Total
   - TIMELIMIT indicate the maximum allowed time for each job (e.g. 3 days)

** squeue
   Check slurm queue

   [Commands]
   $ squeue

   [Output]
   JOBID  NAME          STATE     USER     GROUP    PARTITION  NODE NODELIST(REASON)  CPUS TRES_PER_NODE TIME_LIMIT  TIME_LEFT  
   51617  train-8x2080  RUNNING   aren     usercl4  2080ti     1    n2                1    gpu:8         3-00:00:00  2-07:15:18 
   51575  0.5.4.2.7p4   RUNNING   weebum   usercl4  titanxp    1    n11               2    gpu:1         2-23:00:00  1-00:50:06 
   51556  unfreeze      RUNNING   lee1jun  usercl4  titanxp    1    n10               8    gpu:8         3-00:00:00  7:42:58

   [Details]
   - State: running/pendding/cg
     - pendding: Job is waiting to run
     - cg: The job is done. It disappears soon

   [Other commands]
   $ scontrol show job [job_id]
   $ scontrol show node [running_node_name]

** Run
   - interactive: run shell with resources
   - batch: run batch script with resources
*** Interactive run
    [srun/salloc options]
    - p : partition 지정
    - N : 총 할당받을 노드개수
    - n, --ntasks : 실행할 task 개수
    - t, -time : 최대 실행시간
    --gress=gpu : 노드별 gpu 개수
**** srun only
     2 nodes, 2 gpu per node

     $ srun -p titanxp -N 2 -n 4  -t 00:30:00 --gres=gpu:2 --pty /bin/bash -l  # resource allocation
     $ exit  # exit from queue
**** salloc & srun
     1) Allocate resources with salloc
     2) Run tasks with srun

     $ salloc -N 1 -n 6  -t 00:30:00  /bin/bash -l  # resource allocation
     $ srun execution.sh  # run task with the resources
     $ exit  # exit from queue

*** Batch run
    1) Write script with SBATCH options
    2) run the script with sbatch commands
       $ sbatch tf-2gpu.slurm.sh

    [Example script]
    #+begin_src sh
    #!/bin/sh

    #SBATCH -J  multi-2gpu   	# Job name
    #SBATCH -o  multi-2gpu.%j.out    # Name of stdout output file (%j expands to %jobId)
    #SBATCH -p gpu-titanxp     	# queue  name  or  partiton name gpu-titanxp,gpu-2080ti
    #SBATCH -t 01:30:00 		# Run time (hh:mm:ss) - 1.5 hours

    #### Select  GPU

    ## gpu 2장
    #SBATCH   --gres=gpu:2

    ## gpu 4장
    ##SBATCH   --gres=gpu:4

    ## 노드 지정
    ##SBATCH  --nodelist=n1

    ## 노드 지정하지않기
    #SBATCH   --nodes=1

    ## gpu 가 2장이면  --ntasks=2, --tasks-per-node=2 , --cpus-per-task=1
    ## gpu 가 4장이면  --ntasks=4, --tasks-per-node=4 , --cpus-per-task=1

    #SBTACH   --ntasks=2
    #SBATCH   --tasks-per-node=2
    #SBATCH   --cpus-per-task=1

    cd  $SLURM_SUBMIT_DIR

    echo "SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"
    echo "CUDA_HOME=$CUDA_HOME"
    echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
    echo "CUDA_VERSION=$CUDA_VERSION"


    srun -l /bin/hostname
    srun -l /bin/pwd
    srun -l /bin/date

    module  purge
    module  load   postech


    SAMPLES_DIR=$HOME/TensorFlow-2.x-Tutorials/
    python3  $SAMPLES_DIR/03-Play-with-MNIST/main.py

    date

    squeue  --job  $SLURM_JOBID

    echo  "##### END #####"
    #+end_src

** Stop jobs
   - stop all my jobs
     $ scancel -u [user_id]
   - stop a specific job
     $ scancel [job_id]
