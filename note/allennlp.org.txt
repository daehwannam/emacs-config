
* Abstract Classes
** Vocabulary
   - Vocabulary has several namespaces
   - for each namespace, it has information of relationships between tokens and indices
** TokenIndexer
   - its namespace corresponds to that of Vocabulary
   - the results of indexing tokens can be complex dictionary
     e.g. the output of PretrainedTransformerIndexer include
          - token_ids
          - mask
          - type_ids
*** Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer
    It's a modified code from https://guide.allennlp.org/representing-text-as-features#6

    [setup]
    #+begin_src python
from allennlp.data.fields import TextField
from allennlp.data.token_indexers import (
    PretrainedTransformerIndexer,
)
from allennlp.data.tokenizers import (
    PretrainedTransformerTokenizer,
)
from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder
from allennlp.modules.token_embedders import (
    PretrainedTransformerEmbedder,
)

import warnings
warnings.filterwarnings("ignore")
    #+end_src

    [body]
    #+begin_src python
# model & input &tokenization
text = "Some text with an extraordinarily long identifier."
transformer_model = 'bert-base-cased'
tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)
tokens = tokenizer.tokenize(text)

# tokens to indices
token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)
token_indexers = {'bert_tokens': token_indexer}
text_field = TextField(tokens, token_indexers)
padding_lengths = text_field.get_padding_lengths()
tensor_dict = text_field.as_tensor(padding_lengths)
batched_tensor_dict = text_field.batch_tensors([tensor_dict])

# indices to embeddings
token_embedder = PretrainedTransformerEmbedder(model_name=transformer_model)  # [dhnam]
text_field_embedder = BasicTextFieldEmbedder(token_embedders={'bert_tokens': token_embedder})
embedded_tokens = text_field_embedder(batched_tensor_dict)
    #+end_src

    [result]
    #+begin_example
    tokens: [[CLS], Some, text, with, an, extra, ##ord, ##ina, ##rily, long, id, ##ent, ##ifier, ., [SEP]]
    tensor_dict: {'bert_tokens': {'token_ids': tensor([  101,  1789,  3087,  1114,  1126,  3908,  6944,  2983, 11486,  1263, 25021,  3452, 17792,   119,   102]),
                                  'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]),
				  'type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}}
    padding_lengths: {'bert_tokens___token_ids': 15, 'bert_tokens___mask': 15, 'bert_tokens___type_ids': 15}
    embedded_tokens: tensor([[[ 0.4259,  0.1420,  0.1182,  ..., -0.1887,  0.4505,  0.1054],
                              [ 0.4018, -0.1443,  0.3954,  ..., -0.0100,  0.3334,  0.3635],
                              [ 0.2489,  0.2416,  0.3365,  ...,  0.0949,  0.4748,  0.0973],
                              ...,
                              [ 0.0340, -0.2158,  0.0057,  ...,  0.3255,  0.2093,  0.4995],
                              [ 0.1863,  0.1893, -0.0344,  ..., -0.0134,  0.5061,  0.4039],
                              [ 0.7173,  0.0563, -0.5152,  ...,  0.0256,  0.3780,  0.1898]]],
                            grad_fn=<CatBackward>)
    embedded_tokens.size(): torch.Size([1, 15, 768])
    #+end_example
** token_indexers: Dict[str, TokenIndexer] and its keys
   token_indexers is a dictionary of pairs whose key is a indexer's name and value is a TokenIndexer
*** with Field.get_padding_lengths
    token_indexers' key is used by Field.get_padding_lengths such as TextField.get_padding_lengths
    - TextField.get_padding_lengths
      The key of padding_lengths is combination of keys of token_indexers and TextField._indexed_tokens,
      where the latter is always "tokens" when Field._indexed_tokens made by SingleIdTokenIndexer.tokens_to_indices in Field.index ("tokens" is hard-coded in SingleIdTokenIndexer.tokens_to_indices).
    e.g. Output of get_padding_lengths
    [[*Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer][Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer]]
    >>> text_field.get_padding_lengths()
    ==> {'bert_tokens___token_ids': 15, 'bert_tokens___mask': 15, 'bert_tokens___type_ids': 15}
*** with TextFieldEmbedder
    token_indexers' key is used by TextFieldEmbedder such as BasicTextFieldEmbedder
    - token_indexers: pairs of indexer names and TokenIndexers
    - TextFieldEmbedder: pairs of indexer names and TokenEmbedders
** Field
*** Field.as_tensor
    it returns a nested dictionary that has the same keys of padding_lengths returned by Field.get_padding_lengths
*** TextField vs. LabelField
    TextField and LabelField use different implementations of 'batch_tensors'
    LabelField inherits 'batch_tensors' from Field, so it directly returns 'torch.tensor'
** FromParam
   https://guide.allennlp.org/using-config-files#4
   https://docs.allennlp.org/main/api/models/model/#model-objects
*** Configuration entry
    documentaion of some class, such as Model or Trainer, describes which argument should not get an "entry" in configuration file
    e.g. Model.vocab should not get an entry of a configuration file
** Registrable.register and Lazy
   - when a constructor with a argument of Lazy type annotation is registered by "Registrable.register",
     the argument is created in the constructor rather than complete object is passed as the argument.
   - lazy arguments can be created with some complex process
   - a Lazy object instantiate the specific type object via "Lazy.construct" with additional keyword arguments
*** example with TrainModel.from_partial_objects
    in TrainModel.from_partial_objects, Vocabulary object is passed to Lazy.construct for Model:
    #+begin_src python
vocabulary_ = vocabulary.construct(instances=instance_generator)
if not vocabulary_:
    vocabulary_ = Vocabulary.from_instances(instance_generator)
model_ = model.construct(vocab=vocabulary_)
    #+end_src
*** example 2
    #+begin_src python
import json

from allennlp.common import FromParams, Params, Registrable, Lazy
from allennlp.data import Vocabulary


class Gaussian(FromParams):
    def __init__(self, vocab: Vocabulary, mean: float, variance: float):
        self.vocab = vocab
        self.mean = mean
        self.variance = variance
        print(f"Gaussian got vocab with object id: {id(vocab)}")


class ModelWithGaussian(Registrable):
    def __init__(self, vocab: Vocabulary, gaussian: Gaussian):
        self.vocab = vocab
        self.gaussian = gaussian

    @classmethod
    def from_lazy_objects(cls, gaussian: Lazy[Gaussian]) -> "ModelWithGaussian":
        # Pretend that we needed to do some non-trivial processing / reading from
        # disk in order to construct this object.
        vocab = Vocabulary()
        gaussian_ = gaussian.construct(vocab=vocab)
        return cls(vocab=vocab, gaussian=gaussian_)


# In order to use a constructor other than __init__, we need to inherit from
# Registrable, not just FromParams, and register the class with the separate
# constructor.  And because we're registering the Registrable class itself, we
# can't do this as a decorator, like we typically do.
ModelWithGaussian.register("default", constructor="from_lazy_objects")(
    ModelWithGaussian
)
ModelWithGaussian.default_implementation = "default"


param_str = """{"gaussian": {"mean": 0.0, "variance": 1.0}}"""
params = Params(json.loads(param_str))

model = ModelWithGaussian.from_params(params=params)
print("Mean:", model.gaussian.mean)
print("Variance:", model.gaussian.variance)
    #+end_src
** Early importing Registerable classes
   https://guide.allennlp.org/using-config-files#7
   - argument "--include-package"
   - file ".allennlp_plugins"
* Modules
** Elmo
   - any tokenizer can be used (such as WhitespaceTokenizer), but the same tokenizer used for pre-training is the best.
   - ELMoTokenCharactersIndexer
   - ElmoTokenEmbedder
** Pretrained transformer
   - PretrainedTransformerTokenizer
   - PretrainedTransformerIndexer
   - PretrainedTransformerEmbedder
* Process of running
** processing text to tensors
   - an Instance consists of Fields
   - the length information of a Field can be obtained by Field.get_padding_lengths()
     e.g.
     >>> padding_lengths = field.get_padding_lengths()
     - just as a footnote, LabelField.get_padding_lengths returns an empty dictionary,
       so it doesn't consider padding_lengths
   - for batching, multiple padding_lengths values are merged into an unified padding_lengths
     which has the maximum value for each position
     - the number of padding_lengths values before merging = batch size
   - a Field can be converted into a dictionary of tensors by Field.as_tensor when a padding_lengths is given
     e.g.
     >>> tensor: DataArray = field.as_tensor(padding_lengths)
     >>> tensor: TextFieldTensors = text_field.as_tensor(padding_lengths)
     >>> {'tokens': {'tokens': tensor([ [ 244,   31,  230,  ...,], ...])}}
     where TextFieldTensors = Dict[str, Dict[str, torch.Tensor]]
     - a key of DataArray is an indexer name
   - multiple tensors can be merged into a batched tensor
     e.g.
     >>> batched_tensor: DataArray = field.batch_tensors([tensor1, tensor2, ...])
   - TextFieldEmbedder, such as BasicTextFieldEmbedder, converts TextFieldTensors into torch.Tensor
     - BasicTextFieldEmbedder consists of several TokenEmbedders
       where each TokenEmbedder is specialized for embedding each Field
       - examples of TokenEmbedders include
	 - Embedding
	 - TokenCharactersEncoder
	 - PretrainedTransformerEmbedder
	 - and so on
   - usually, TextFieldEmbedder is used for each argument of Model.forward
     - the argument is a batched tensor for a Field
       - It could be a TextFieldTensor (for features)
         or a torch.IntTensor (sometimes for labels)
     - each argument's name corresponds to each field name of an Instance
** Batching
   - Instance also has methods Instance.get_padding_lengths and
     Instance.as_tensor_dict, which are used for converting an Instance into tensor_dict
     - Instance.get_padding_lengths uses Field.get_padding_lengths
       - however, resulted padding_lengths doesn't include the lengths of LabelField
     - Instance.as_tensor_dict uses Field.as_tensor
       - the result of Instance.as_tensor_dict is the dictionary
  	 from pairs of field_name and field.as_tensor(...)
  	 - it's type is Dict[str, DataArray]

* Configuration
** allennlp.commands.TrainModel.from_partial_objects
   configuration file for 'allennlp train' command corresponds to the parameters of 'allennlp.commands.TrainModel.from_partial_objects' except the first three parameters
** Random seed example
   https://guide.allennlp.org/hyperparameter-optimization#3
   {numpy_seed: seed,
    pytorch_seed: seed,
    random_seed: seed, ...}
