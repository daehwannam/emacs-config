
* Abstract Classes
** Vocabulary
   - Vocabulary has several namespaces
   - for each namespace, it has information of relationships between tokens and indices
** TokenIndexer
   - its namespace corresponds to that of Vocabulary
   - the results of indexing tokens can be complex dictionary
     e.g. the output of PretrainedTransformerIndexer include
          - token_ids
          - mask
          - type_ids
*** Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer
    It's a modified code from https://guide.allennlp.org/representing-text-as-features#6

    [code]
    #+begin_src python
from allennlp.data import Vocabulary
from allennlp.data.fields import TextField
from allennlp.data.token_indexers import (
    PretrainedTransformerIndexer,
)
from allennlp.data.tokenizers import (
    PretrainedTransformerTokenizer,
)
from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder
from allennlp.modules.token_embedders import (
    PretrainedTransformerEmbedder,
)

import warnings
warnings.filterwarnings("ignore")


# model & input &tokenization
text = "Some text with an extraordinarily long identifier."
transformer_model = 'bert-base-cased'
tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)
tokens = tokenizer.tokenize(text)
print("tokens:", tokens)

# tokens to indices
vocab = Vocabulary()
token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)
token_indexers = {'bert_tokens': token_indexer}
text_field = TextField(tokens, token_indexers)
text_field.index(vocab)
padding_lengths = text_field.get_padding_lengths()
print("padding_lengths:", padding_lengths)
tensor_dict = text_field.as_tensor(padding_lengths)
print("tensor_dict:", tensor_dict)
batched_tensor_dict = text_field.batch_tensors([tensor_dict])

# indices to embeddings
token_embedder = PretrainedTransformerEmbedder(model_name=transformer_model)  # [dhnam]
text_field_embedder = BasicTextFieldEmbedder(token_embedders={'bert_tokens': token_embedder})
embedded_tokens = text_field_embedder(batched_tensor_dict)
print("embedded_tokens:", embedded_tokens)
print("embedded_tokens.size():", embedded_tokens.size())
    #+end_src

    [result]
    #+begin_example
    tokens: [[CLS], Some, text, with, an, extra, ##ord, ##ina, ##rily, long, id, ##ent, ##ifier, ., [SEP]]
    padding_lengths: {'bert_tokens___token_ids': 15, 'bert_tokens___mask': 15, 'bert_tokens___type_ids': 15}
    tensor_dict: {'bert_tokens': {'token_ids': tensor([  101,  1789,  3087,  1114,  1126,  3908,  6944,  2983, 11486,  1263, 25021,  3452, 17792,   119,   102]),
                                  'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]),
				  'type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}}
    embedded_tokens: tensor([[[ 0.4259,  0.1420,  0.1182,  ..., -0.1887,  0.4505,  0.1054],
                              [ 0.4018, -0.1443,  0.3954,  ..., -0.0100,  0.3334,  0.3635],
                              [ 0.2489,  0.2416,  0.3365,  ...,  0.0949,  0.4748,  0.0973],
                              ...,
                              [ 0.0340, -0.2158,  0.0057,  ...,  0.3255,  0.2093,  0.4995],
                              [ 0.1863,  0.1893, -0.0344,  ..., -0.0134,  0.5061,  0.4039],
                              [ 0.7173,  0.0563, -0.5152,  ...,  0.0256,  0.3780,  0.1898]]],
                            grad_fn=<CatBackward>)
    embedded_tokens.size(): torch.Size([1, 15, 768])
    #+end_example
** token_indexers: Dict[str, TokenIndexer] and its keys
   token_indexers is a dictionary of pairs whose key is a indexer's name and value is a TokenIndexer
*** with Field.get_padding_lengths
    token_indexers' key is used by Field.get_padding_lengths such as TextField.get_padding_lengths
    - TextField.get_padding_lengths
      The key of padding_lengths is combination of keys of token_indexers and TextField._indexed_tokens,
      where the latter is always "tokens" when Field._indexed_tokens made by SingleIdTokenIndexer.tokens_to_indices in Field.index ("tokens" is hard-coded in SingleIdTokenIndexer.tokens_to_indices).
    e.g. Output of get_padding_lengths
         [[*Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer][Example: using PretrainedTransformerIndexer with PretrainedTransformerTokenizer]]
         >>> text_field.get_padding_lengths()
         ==> {'bert_tokens___token_ids': 15, 'bert_tokens___mask': 15, 'bert_tokens___type_ids': 15}
*** with TextFieldEmbedder
    token_indexers' key is used by TextFieldEmbedder such as BasicTextFieldEmbedder
    - token_indexers: pairs of indexer names and TokenIndexers
    - TextFieldEmbedder: pairs of indexer names and TokenEmbedders
** Field
*** Field.as_tensor
    it returns a nested dictionary that has the same keys of padding_lengths returned by Field.get_padding_lengths
*** TextField vs. LabelField
    TextField and LabelField use different implementations of 'batch_tensors'
    LabelField inherits 'batch_tensors' from Field, so it directly returns 'torch.Tensor'
*** MetadataField
    - A MetadataField is a Field that does not get converted into torch.Tensor
    - MetadataField.as_tensor returns the object which is passed to its initializer
** FromParam
   https://guide.allennlp.org/using-config-files#4
   https://docs.allennlp.org/main/api/models/model/#model-objects
*** Configuration entry
    documentaion of some class, such as Model or Trainer, describes which argument should not get an "entry" in configuration file
    e.g. Model.vocab should not get an entry of a configuration file
** Registrable
*** Registrable.register and Lazy
    - when a constructor with a argument of Lazy type annotation is registered by "Registrable.register",
      the argument is created in the constructor rather than complete object is passed as the argument.
    - lazy arguments can be created with some complex process
    - a Lazy object instantiate the specific type object via "Lazy.construct" with additional keyword arguments
**** example with TrainModel.from_partial_objects
     in TrainModel.from_partial_objects, Vocabulary object is passed to Lazy.construct for Model:
     #+begin_src python
vocabulary_ = vocabulary.construct(instances=instance_generator)
if not vocabulary_:
    vocabulary_ = Vocabulary.from_instances(instance_generator)
model_ = model.construct(vocab=vocabulary_)
     #+end_src
**** example 2
     #+begin_src python
import json

from allennlp.common import FromParams, Params, Registrable, Lazy
from allennlp.data import Vocabulary


class Gaussian(FromParams):
    def __init__(self, vocab: Vocabulary, mean: float, variance: float):
        self.vocab = vocab
        self.mean = mean
        self.variance = variance
        print(f"Gaussian got vocab with object id: {id(vocab)}")


class ModelWithGaussian(Registrable):
    def __init__(self, vocab: Vocabulary, gaussian: Gaussian):
        self.vocab = vocab
        self.gaussian = gaussian

    @classmethod
    def from_lazy_objects(cls, gaussian: Lazy[Gaussian]) -> "ModelWithGaussian":
        # Pretend that we needed to do some non-trivial processing / reading from
        # disk in order to construct this object.
        vocab = Vocabulary()
        gaussian_ = gaussian.construct(vocab=vocab)
        return cls(vocab=vocab, gaussian=gaussian_)


# In order to use a constructor other than __init__, we need to inherit from
# Registrable, not just FromParams, and register the class with the separate
# constructor.  And because we're registering the Registrable class itself, we
# can't do this as a decorator, like we typically do.
ModelWithGaussian.register("default", constructor="from_lazy_objects")(
    ModelWithGaussian
)
ModelWithGaussian.default_implementation = "default"


param_str = """{"gaussian": {"mean": 0.0, "variance": 1.0}}"""
params = Params(json.loads(param_str))

model = ModelWithGaussian.from_params(params=params)
print("Mean:", model.gaussian.mean)
print("Variance:", model.gaussian.variance)
     #+end_src
*** Early importing Registerable classes
    https://guide.allennlp.org/using-config-files#7
    - argument "--include-package"
    - file ".allennlp_plugins"
** Instance
*** Instance.get_padding_lengths
     - Instance.get_padding_lengths uses Field.get_padding_lengths
       - however, resulted padding_lengths doesn't include the lengths of LabelField
     e.g.
     [code]
     #+begin_src python
from collections import Counter, defaultdict

from allennlp.data.instance import Instance
from allennlp.data.fields import TextField, LabelField, SequenceLabelField
from allennlp.data.token_indexers import SingleIdTokenIndexer
from allennlp.data.tokenizers import Token
from allennlp.data.vocabulary import Vocabulary


# Create Fields
tokens = [Token('The'), Token('best'), Token('movie'), Token('ever'), Token('!')]
token_indexers = {'token_indexer': SingleIdTokenIndexer('token_namespace')}  # [dhnam]
text_field = TextField(tokens, token_indexers=token_indexers)

label_field = LabelField('pos', label_namespace='lab')  # [dhnam]

sequence_label_field = SequenceLabelField(
    ['DET', 'ADJ', 'NOUN', 'ADV', 'PUNKT'],
    text_field,
    label_namespace='lab'
)  # [dhnam]

# Create an Instance
fields = {
    'text_field': text_field,
    'label_field': label_field,
}
instance = Instance(fields)

# You can add fields later
instance.add_field('label_seq', sequence_label_field)

# You can simply use print() to see the instance's content
print(instance)

# Create a Vocabulary
counter = defaultdict(Counter)
instance.count_vocab_items(counter)
vocab = Vocabulary(counter)

# Convert all strings in all of the fields into integer IDs by calling index_fields()
instance.index_fields(vocab)

# Instances know how to turn themselves into a dict of tensors.  When we call this
# method in our data code, we additionally give a `padding_lengths` argument.
# We will pass this dictionary to the model as **tensors, so be sure the keys
# match what the model expects.
tensors = instance.as_tensor_dict()
print(tensors)
     #+end_src

     [result]
     #+begin_example
     {'text_field': {'token_indexer': {'tokens': tensor([2, 3, 4, 5, 6])}}, 'label_field': tensor(2), 'label_seq': tensor([3, 4, 5, 6, 7])}
     #+end_example

     [warnings]
     #+begin_example
     Your label namespace was 'lab'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.

     Your label namespace was 'lab'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.
     #+end_example
*** Instance.as_tensor_dict
     - Instance.as_tensor_dict uses Field.as_tensor
       - the result of Instance.as_tensor_dict is the dictionary
  	 from pairs of field_name and field.as_tensor(...)
  	 - it's type is Dict[str, DataArray]

* Modules
** Elmo
   - any tokenizer can be used (such as WhitespaceTokenizer), but the same tokenizer used for pre-training is the best.
   - ELMoTokenCharactersIndexer
   - ElmoTokenEmbedder
** Pretrained transformer
   - PretrainedTransformerTokenizer
   - PretrainedTransformerIndexer
   - PretrainedTransformerEmbedder
* Process of running
** processing text to tensors
   - an Instance consists of Fields
   - the length information of a Field can be obtained by Field.get_padding_lengths()
     e.g.
     >>> padding_lengths = field.get_padding_lengths()
     - just as a footnote, LabelField.get_padding_lengths returns an empty dictionary,
       so it doesn't consider padding_lengths
   - for batching, multiple padding_lengths values are merged into an unified padding_lengths
     which has the maximum value for each position
     - the number of padding_lengths values before merging = batch size
   - a Field can be converted into a dictionary of tensors by Field.as_tensor when a padding_lengths is given
     e.g.
     >>> tensor: DataArray = field.as_tensor(padding_lengths)
     >>> tensor: TextFieldTensors = text_field.as_tensor(padding_lengths)
     >>> {'tokens': {'tokens': tensor([ [ 244,   31,  230,  ...,], ...])}}
     where TextFieldTensors = Dict[str, Dict[str, torch.Tensor]]
     - a key of DataArray is an indexer name
   - multiple tensors can be merged into a batched tensor
     e.g.
     >>> batched_tensor: DataArray = field.batch_tensors([tensor1, tensor2, ...])
   - TextFieldEmbedder, such as BasicTextFieldEmbedder, converts TextFieldTensors into torch.Tensor
     - BasicTextFieldEmbedder consists of several TokenEmbedders
       where each TokenEmbedder is specialized for embedding each Field
       - examples of TokenEmbedders include
	 - Embedding
	 - TokenCharactersEncoder
	 - PretrainedTransformerEmbedder
	 - and so on
   - usually, TextFieldEmbedder is used for each argument of Model.forward
     - the argument is a batched tensor for a Field
       - It could be a TextFieldTensor (for features)
         or a torch.IntTensor (sometimes for labels)
     - each argument's name corresponds to each field name of an Instance
** Batching
   - Instance also has methods Instance.get_padding_lengths and
     Instance.as_tensor_dict, which are used for converting an Instance into tensor_dict
     - Instance.get_padding_lengths uses Field.get_padding_lengths
       - however, resulted padding_lengths doesn't include the lengths of LabelField
     - Instance.as_tensor_dict uses Field.as_tensor
       - the result of Instance.as_tensor_dict is the dictionary
  	 from pairs of field_name and field.as_tensor(...)
  	 - it's type is Dict[str, DataArray]

* Configuration
** allennlp.commands.TrainModel.from_partial_objects
   configuration file for 'allennlp train' command corresponds to the parameters of 'allennlp.commands.TrainModel.from_partial_objects' except the first three parameters
** Random seed example
   https://guide.allennlp.org/hyperparameter-optimization#3
   {numpy_seed: seed,
    pytorch_seed: seed,
    random_seed: seed, ...}
