
* convergence example
** Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision
   - convergence with adam
   - convergence with learning rate decay
** From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood
   - "train until accuracy on the validation set converges"

* early stoping
  - patience-based approach: it stops when the best performance(ex. accuracy) over development set is not updated during 'patience' number of epochs
** materials
   - https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/
   - https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/
     - it contains examples of early stopping
     - sometimes early stopping is not work:
       "Understanding deep learning requires rethinking generalization"
       it's when data is abundant & 'patience' is too small
       (but if 'patience' is too big, training needs much time)
** papers dont use ealy stopping?
   https://www.reddit.com/r/MachineLearning/comments/9omr67/discussion_early_stopping_why_not_always/
*** what's early stopping
    Absolutely! But from my understanding, the point of early-stopping is to prevent overfitting. Ideally, early-stopping stops training at the point where the generalization capability of the model is at its highest, and before overfitting starts (save best weights, stop training if no improvement in desired metric in the past N epochs).
** Neural Semantic Parsing with Type Constraints for Semi-Structured Tables
   - it uses dev set to early stop
*** early stopping definition
    - https://en.wikipedia.org/wiki/Early_stopping#Validation-based_early_stopping
**** Goodfellow book
     - section 7.8: early stopping
       http://www.deeplearningbook.org/contents/regularization.html
       (stackoverflow question: https://stats.stackexchange.com/questions/305452/understanding-early-stopping-in-neural-networks-and-its-implications-when-using)
     - validation set & cross validation
       http://www.deeplearningbook.org/contents/ml.html

* best perform on dev set
** Weakly Supervised Semantic Parsing with Abstract Examples
   - "best model W.+DISC on the development set."

* k-fold cross validation
** Neural Semantic Parsing with Type Constraints for Semi-Structured Tables
   this paper uses it
** hyperparameter turing by k-fold cross validation
   https://stats.stackexchange.com/a/43138
** standard error of cross validation
   standard_error_of_cross_validation = standard_deviation_of_k_performance / sqrt(k)  ; k is the number of folds
   - https://stackoverflow.com/questions/34914229/which-standard-deviation-of-the-cross-validation-score
   - http://www.math.canterbury.ac.nz/~r.vale/Crossvalidation.pdf
   - https://stackoverflow.com/questions/34914229/which-standard-deviation-of-the-cross-validation-score
** nested k-fold cross validation
   - https://stats.stackexchange.com/questions/56421/is-it-ok-to-determine-early-stopping-using-the-validation-set-in-10-fold-cross-v
   - https://stats.stackexchange.com/questions/90288/in-k-fold-cross-validation-does-the-training-subsample-include-test-set
** application of k-fold cross validation
   - reliably testing model performance
   - model selection or hyper parameter tuning
   - ensembling k number of models
** Joint Concept Learning and Semantic Parsing from Natural Language Explanations
   10-fold cross validation to evaluate test performance rather than model selection

* ensemble model
** Neural Semantic Parsing with Type Constraints for Semi-Structured Tables
   - it averages logical form probabilities of 5 models trained by each fold
     (rather than averages action probabilities)

* language model
** n-gram model derivation
   https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/
   proof by using Lagrange multiplier

* A* search
** Why does A* with admissible non consistent heuristic find non optimal solution?
   https://stackoverflow.com/questions/51684682/why-does-a-with-admissible-non-consistent-heuristic-find-non-optimal-solution
** Optimality when heuristic is admissible / consistent
   - https://en.wikipedia.org/wiki/Admissible_heuristic#Notes
   - https://ai.stackexchange.com/questions/6026/admissible-consistent-heuristic-theorems
   - AI modern approach Section 3.5.2 Optimality of A* (page 95)
     As we mentioned earlier, A* has the following properties: the tree-search version of A* is optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.
* Computer network
  - Exponential backoff

* definition of machine learning
  - https://cedar.buffalo.edu/~srihari/CSE574/Chap1/Machine-Learning-Intro-old.pdf
  - Tom M. Mitchell
    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
    - 'experience' means 'data'
      ex. data in supervised learning or data collected during reinforcement learning

* Beam search
** efficient top-k selection (instead of sorting)
   - https://stackoverflow.com/questions/35146460/time-complexity-and-space-complexity-for-beam-search
   - https://en.wikipedia.org/wiki/Quickselect#Variants
   - https://en.wikipedia.org/wiki/Introselect

* consistent / spurious logical form
  (Macro Grammars and Holistic Triggering for Efficient Semantic Parsing)
  - logical form produce correct denotation --> consistent
  - logical form produce correct denotation but it don't reflect natural language utterance
    --> the program is consistent with respect to denotation but it's spurious

* Policy gradient
** Why does baseline work?
   https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/
   baseline reduces the approximated variance of gradients

* Type theory
  https://en.wikipedia.org/wiki/Type_theory#Linguistics

* Semantic parsing dataset
  https://github.com/allenai/acl2018-semantic-parsing-tutorial

* Gumbel max trick
** Sampling from probability density function with uniform distribution
   - f(x): probability density function (PDF)
   - F(x): cumulative distribution function (CDF)
   - F^-1(p): inverse function of CDF, whose domain is [0, 1]
   - Uniform(0, 1): uniform distribution from 0 to 1

   p ~ Uniform(0, 1)
   x' = F^-1(p) ; it's same that x' is sampled from f(x)
** Sampling from Gumbel distribution
   Gumbel distribution requires location mu and scale beta.
   We assume scale beta = 1

   f(x; mu) = exp(-((x - mu) + exp(-(x - mu))))
   F(x; mu) = exp(-exp(-(x - mu)))
   F^-1(p; mu) = mu - log(-log(p))

   p ~ Uniform(0, 1)
   x' = F^-1(p; mu) ; it's same that x' is sampled from f(x; mu) or Gumbel(mu, 1)
** Gumbel max statistics(stability) trick
   - Reference
     - "On the Partition Function and Random Maximum A-Posteriori Perturbations"
        https://arxiv.org/abs/1206.6410
   # \begin{align*}
   # G_k + \log \alpha_k &= \max\{G_i + \log \alpha_i) \\
   # \mathbb{P}(X=\max\{G_i + \log \alpha_i)\}
   # &= \prod_{i \ne k} \mathbb{P}(X \ge G_i + \log \alpha_i) \\
   # &= \prod_{i \ne k} \mathbb{P}(G_i \le X -\log \alpha_i) \\
   # &= \prod_{i \ne k} F(X -\log \alpha_i) \\
   # &= ... then-how?
   # \end{align*}

   $\max\{G_i + \log \alpha_i)$ is a random variable and C is a constant
   \begin{align*}
   \mathbb{P}(\max\{G_i + \log \alpha_i) \le C\}
   &= \prod_{i=1..n} \mathbb{P}(G_i + \log \alpha_i \le C) \\
   &= \prod_{i=1..n} \mathbb{P}(G_i \le C -\log \alpha_i) \\
   &= \prod_{i=1..n} F(C -\log \alpha_i) \\
   &= (skip) \\
   &= F(C -\log \sum_{i=1..n} \alpha_i)
   \end{align*}

   $F(C -\log \sum_{i=1..n} \alpha_i)$ is the cumulative distribution of $Gumbel(\log \sum_{i=1..n}\alpha_i)$

   
** Gumbel arg-max trick proof
   https://i.hsfzxjy.site/2019-08-01-proof-of-gumbel-max-trick/
   https://medium.com/swlh/on-the-gumbel-max-trick-5e340edd1e01

   Assume:
     \[\sum_{k=1}^n \alpha_k = 1\] and \[0 \leq \alpha_k \leq 1\]
     \[G_k \sim Gumbel(0, 1)\] for each k=1,..,n

   Let \[u_k=\log{\alpha_k}+G_k\]

   \begin{align*}
     \mathbb{P}(Z=k)&=\mathbb{P}(u_k \geq u_j,\forall j \neq k)\\
     &=\int_{-\infty}^\infty \mathbb{P}(u_k \geq u_j, \forall j \neq k|u_k)\mathbb{P}(u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}\mathbb{P}(u_k \geq u_j|u_k)\mathbb{P}(u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}\mathbb{P}(0 \leq u_k - u_j|u_k)\mathbb{P}(u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}\mathbb{P}(0 \leq u_k - (\log \alpha_k + G_k)|u_k)\mathbb{P}(u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}e^{-e^{-u_k+\log \alpha_j}} \mathbb{P}(u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}e^{-e^{-u_k+\log \alpha_j}} \mathbb{P}(U_k=u_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}e^{-e^{-u_k+\log \alpha_j}} \mathbb{P}(G_k=u_k-\log\alpha_k) du_k\\
     &=\int_{-\infty}^\infty \prod_{j\neq k}e^{-e^{-u_k+\log \alpha_j}} e^{-(u_k-\log\alpha_k+e^{-(u_k-\log\alpha_k)})} du_k\\
     &=\int_{-\infty}^\infty e^{-\sum_{j\neq k}\alpha_je^{-u_k}} \alpha_k e^{-(u_k+\alpha_k e^{-u_k})} du_k\\
     &=\alpha_k \int_{-\infty}^\infty e^{-u_k-(\alpha_k+\sum_{j\neq k}\alpha_j)e^{-u_k}} du_k\\
     &=\alpha_k \int_{-\infty}^\infty e^{-u_k-e^{-u_k}} du_k\\
     &= \alpha_k\\
   \end{align*}

   - computing the integral part
     \[\int_{-\infty}^\infty e^{-x-e^{-x}} dx = 1\]
     - https://www.integral-calculator.com/
       - expression: exp(-x-exp(-x))
       - range: -infinity ~ +infinity

** Truncated Gumbel distribution
   - Reference
     - "On the Partition Function and Random Maximum A-Posteriori Perturbations"
     - "Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement"

* Reinforcement learning basic
** Model-based vs. model-free
   https://ai.stackexchange.com/a/4465
   - model-based: a model tries to capture the environment, so it estimates transition function and reward function
   - model-free: a model doesn't estimate transition function and reward function
** Monte-carlo vs. temporal-difference
   https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods
   - monte-carlo: learning from complete episodes
   - temporal-difference: learning from incomplete episodes
** On-policy vs. off-policy
   https://stats.stackexchange.com/a/184794
   - learning from episodes sampled from the current policy (such as SARSA)
   - learning from episodes sampeld from another policy (such as Q-learning)
** Epsilon-greedy
   - epsilon-greedy is used for off-policy algorithms
   - epsilon-greedy should not be used with policy gradient methods
     - howevery epsilon-greedy policy can be used with off-policy policy gradients

* Writing
** reference
*** et al. notation
    https://www.scribendi.com/academy/articles/how_to_use_et_al.en.html
    - Lucas et al. (1995) explores...
    - (Lucas et al., 1995)
